---
layout: single
title: 📚동영상 AI의 발전📚
---
## 📢Briefing
오늘은 동영상 AI 모델들에 대해 공부해보았다.

## 🛠동영상 AI의 시작

초기에 GAN 모델이라는 이미지 생성 모델이 있었는데 이를 통해 연속된 영상을 만들려고 하는 시도들이 있었다.

하지만 GAN은 이미지를 생성하는 것만으로도 벅찬 모델이었기 때문에 더이상의 발전은 기대하지 어려웠다.


## 🛠동영상 AI의 발전

그러다 최근 Diffusion model이라는 이미지 생성 모델이 나오게 되면서 다시 한번 동영상 AI의 발전을 기대해볼 수 있게 되었다. 
방식은 이러하다. “video to video”라는 프로그램을 통해 프레임별로 이미지를 끊었고, 
이 이미지들을 Diffusion에 입력해서 나오는 이미지들을 다시 영상으로 만드는 일종의 수공업과 같은 형태이다. 이를 본 대기업들은 동영상을 직접 만드는 AI를 만들기에 돌입하였다.

대표적인 예로 구글의 IMAGEN, Meta의 Makeavideo가 있다. 
이 프로그램들은 Diffusion 모델로 이미지를 여러장 뽑을 수 있게 훈련되어있다. 
여기까지만 실행하면 저화질에 다소 끊기는 느낌이 드는 영상이 만들어진다. 
여기에 화질을 올리고, 프레임을 늘여서 영상을 만들게 되면 보다 자연스러운 영상이 만들어지게 된다.


## 🛠최신 발전 방향

최근에는 GPT에게 영상가이드를 맡기는 시도도 늘어나고 있는 듯하다.

또 LLM을 통해서 영상을 만드는 기술도 발전하고 있다. 여기서 LLM이란 large language model로, **다양한 자연어 처리 작업을 수행할 수 있는 딥러닝 알고리즘**이다.

LLM을 사용한 예로는 Google이 만든 **VideoPoet**이 있다(글을 쓰는 시점에서 아직 상용화되지 않음).

비슷한 프로그램으로는 Runway Gen 2 모델이나 Pikalabs를 사용해보는 것을 추천한다.


## 🛠어떻게 활용할 수 있을까?

그렇다면 이러한 모델들은 어떨때 활용하면 좋을까? 예를 들어 어떤 이미지에 있는 문구에 효과를 주고 싶다면,

해당 이미지를 모델에게 주고, 원하는 효과를 입력하게 되면 특별한 수고 없이 원하는 효과의 영상을 얻을 수 있게 되는 것이다.

또 이 기술들은 발전하여 현재에는 인페이팅(inpainting) 기술로, 영상의 특정부분만 편집할 수 있는 기술도 갖추게 되었다.

앞서 설명한 VideoPoet은 음성도 넣을 수 있다고 한다.


## 🛠마무리 지으며..

지금은 실제로 활용하기에는 미비한 부분들이 다소 있는 기술들이다. 
하지만 지금처럼 계속해서 발전을 거듭해나간다면 앞으로는 영상계에 새로운 바람이 불어오게 될지도 모르는 일이다. 
기술의 발전으로 삶이 윤택해지는 것은 좋으나 그 기술을 윤리적으로 잘 활용하는 것이 중요할 것이다.

--

**🔍참고한 사이트:** <br>
[YOUTUBE](https://www.youtube.com/watch?v=lZZ19fLWjfI)
